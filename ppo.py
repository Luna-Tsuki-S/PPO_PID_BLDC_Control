
"""

# !pip install stable-baselines3 gym numpy matplotlib shimmy
# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# !pip install --upgrade stable-baselines3 gymnasium
# !pip install scipy matplotlib pandas


"""# Temp / Humid"""

import gym
import numpy as np
import pandas as pd
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# ç’°å¢ƒï¼ˆä½¿ç”¨ CSVï¼‰
class THBLDC(gym.Env):
    def __init__(self, csv_path):
        self.data = pd.read_csv(csv_path)
        self.max_steps = len(self.data)

        # ç‹€æ…‹ç©ºé–“ï¼š2 ç¶­é€£çºŒæ•¸å€¼ï¼ˆæ¨™æº–åŒ–ï¼‰
        self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # å‹•ä½œç©ºé–“ï¼š0 to 1650 rpm
        self.action_space = spaces.Discrete(1651)
        self.current_step = 0

    def reset(self):
        self.current_step = 0
        return self._get_obs()

    def _get_obs(self):
        row = self.data.iloc[self.current_step]
        return np.array([
            row['Temperature'] / 100,
            row['Humidity'] / 100
        ], dtype=np.float32)

    def step(self, action):
        row = self.data.iloc[self.current_step]
        done = False
        reward = 0



        temp = row['Temperature']
        humid = row['Humidity']

        # RPM rawl
        kwh = ((1650 * (action / 1650) ** 3) * 0.1) / 1000
        tr_fan = 0.006
        save_rate = (kwh - tr_fan) / tr_fan


        # ç¯€èƒ½æ•ˆæœï¼ˆå¸Œæœ› kWh è¶Šå°‘è¶Šå¥½ï¼‰
        reward -= kwh  # ç›´æ¥æ‡²ç½°åŠŸè€—

        # æº«åº¦èˆ’é©åº¦ï¼ˆ22â€“28Â°C ç‚ºèˆ’é©ç¯„åœï¼‰
        reward -= abs(temp - 25) * 0.1  # è¶Šæ¥è¿‘ 25Â°C è¶Šå¥½

        # æ¿•åº¦èˆ’é©åº¦ï¼ˆ40â€“60% ç‚ºèˆ’é©ç¯„åœï¼‰
        reward -= abs(humid - 50) * 0.05  # è¶Šæ¥è¿‘ 50% è¶Šå¥½

        # è‹¥åŠŸè€—æ¯”åŸºæº–çœé›» 30% ä»¥ä¸Šï¼Œé¡å¤–åŠ åˆ†
        if save_rate >= 0.3:
            reward += 0.5

        # è‹¥é¢¨é€Ÿåœ¨èˆ’é©é¢¨é€Ÿå€é–“ï¼ˆex. 500~900 RPMï¼‰ï¼Œé¡å¤–åŠ åˆ†
        if 500 <= action <= 900:
            reward += 0.2

        # å®‰å…¨é¢¨é€Ÿ <= 1300 RPM
        if action <= 1300:
            reward += 0.2

        self.current_step += 1
        if self.current_step >= self.max_steps - 1:
            done = True

        return self._get_obs(), reward, done, {}

    def render(self, mode='human'):
        row = self.data.iloc[self.current_step]
        print(f"Step {self.current_step} - Temp: {row['temp']}Â°C, Humid: {row['humid']}%")

# === è¨“ç·´æ¨¡å‹ ===
csv_path = "/content/temperature_humidity_log.csv"
# csv_path = "/content/sample_data/temperature_humidity_2000.csv"  # æ›¿æ›ç‚ºä½ çš„ CSV æª”è·¯å¾‘
env = DummyVecEnv([lambda: THBLDC(csv_path)])
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=50000)

# === å„²å­˜æ¨¡å‹ ===
model.save("ppo_model")
print("âœ… æ¨¡å‹å·²å„²å­˜ç‚º ppo_model.zip")

def test_model(model, env, episodes=500):
    total_rewards = []
    for ep in range(episodes):
        obs = env.reset()
        done = False
        ep_reward = 0
        while not done:
            action, _ = model.predict(obs)
            obs, reward, done, _ = env.step(action)
            ep_reward += abs(reward)
        total_rewards.append(ep_reward)
        print(f"Episode {ep+1}: Score = {abs(ep_reward)}")
    print(f"\nğŸ“Š Average Score: {np.mean(total_rewards)}")

# === æ¸¬è©¦æ¨¡å‹åŸ·è¡Œ ===
test_model(model, env)

# è®€å–æ¨¡å‹
model = PPO.load("ppo_model.zip")

# å–®ç­†è³‡æ–™ï¼ˆç¯„ä¾‹ï¼‰
#  Temperature, Humidity
current_status = [26, 40]


obs = np.array([
    current_status[0] / 100,
    current_status[1] / 100
], dtype=np.float32)



  # æ¨¡å‹é æ¸¬
action, _ = model.predict(obs, deterministic=True)


print(obs, action)